# Phase 4 D05: FNO-UNet + GELU
experiment_name: "exp2_D05_unet_gelu"
description: "Activation sweep: FNO-UNet with GELU activation (smoother gradients)."

model:
  name: "fno_unet"
  output_dim: 5
  modes: 32

training:
  epochs: 250
  batch_size: 16
  learning_rate: 0.001
  optimizer: "adam"
  scheduler: "plateau"
  scheduler_patience: 15

data:
  resolution: 256
  train_samples: 2000
  val_samples: 400

grid_strategy: "random_all"
standardize_outputs: true
loss_function: "weighted_standardized"
loss_weights: [1.0, 1.0, 1.0, 10.0, 10.0]
activation: "gelu"
